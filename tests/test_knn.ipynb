{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b5f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded. PyTorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports and Configuration\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ART Imports (Adversarial Robustness Toolbox)\n",
    "from art.estimators.classification import SklearnClassifier, PyTorchClassifier\n",
    "from art.attacks.evasion import HopSkipJump, FastGradientMethod\n",
    "\n",
    "# Import KNN model script\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "try:\n",
    "    from scripts.models.knn import train_knn \n",
    "except ImportError:\n",
    "    # Fallback for standalone testing if script is missing\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    def train_knn(X, y, n_neighbors=5, cv=5, logger=None):\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        knn.fit(X, y)\n",
    "        return knn, []\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Libraries loaded. PyTorch device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c500569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UNSW-NB15 dataset (small size for testing)...\n",
      "[INFO] Downloading dataset: mrwellsdavid/unsw-nb15\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/mrwellsdavid/unsw-nb15?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149M/149M [00:39<00:00, 3.91MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Dataset downloaded to: C:\\Users\\pauli\\.cache\\kagglehub\\datasets\\mrwellsdavid\\unsw-nb15\\versions\\1\n",
      "[INFO] Loaded UNSW-NB15_1.csv with shape: (700000, 46)\n",
      "[INFO] DataFrame shape: (700000, 46)\n",
      "[INFO] Initial dimensions: 700,000 rows x 46 columns = 32,200,000 cells\n",
      "[DEBUG] Cleaning column names\n",
      "[DEBUG] Removing duplicate rows\n",
      "[DEBUG] Removed 59,342 duplicate rows. Remaining: 640,658\n",
      "[DEBUG] Removing rows with missing values (initial pass)\n",
      "[DEBUG] Removed 0 rows with missing values. Remaining: 640,658\n",
      "[DEBUG] Checking for infinite values in numeric columns\n",
      "[DEBUG] Missing values before processing infinite values: 0\n",
      "[DEBUG] Missing values after processing infinite values: 0\n",
      "[DEBUG] Infinite values converted to NaN: 0\n",
      "[DEBUG] Dropping 0 columns with only one unique value:\n",
      "[INFO] ============================================================\n",
      "[INFO] Preprocessing completed successfully\n",
      "[INFO] Final dimensions: 640,658 rows x 46 columns\n",
      "[INFO] Total rows removed: 59,342 (8.48%)\n",
      "[INFO] data retention rate: 91.52%\n",
      "[INFO] ============================================================\n",
      "[INFO] Optimizing memory usage of the dataset...\n",
      "[INFO] Initial memory usage: 229.73 MB\n",
      "[INFO] Optimized memory usage: 136.86 MB\n",
      "[INFO] Memory reduction: 92.87 MB (40.43%)\n",
      "[INFO] Encoding attack labels...\n",
      "[DEBUG] Data Labels after encoding:\n",
      "[DEBUG]   Benign: 626380\n",
      "[DEBUG]   Exploits: 4042\n",
      "[DEBUG]    Fuzzers: 3991\n",
      "[DEBUG]   Generic: 2833\n",
      "[DEBUG]   Reconnaissance: 1740\n",
      "[DEBUG]   DoS: 825\n",
      "[DEBUG]   Analysis: 301\n",
      "[DEBUG]   Backdoors: 299\n",
      "[DEBUG]   Shellcode: 223\n",
      "[DEBUG]   Worms: 24\n",
      "[INFO] Attack labels encoded using LabelEncoder() encoder.\n",
      "[INFO] Scaling dataset features...\n",
      "[INFO] Features scaled using MinMaxScaler() scaler.\n",
      "[INFO] Splitting dataset into training and testing sets...\n",
      "[INFO] Class distribution after SMOTE:\n",
      "[INFO]   Class 0: 469773 samples\n",
      "[INFO]   Class 1: 10720 samples\n",
      "UNSW-NB15 data ready. Train shape: (480493, 45), Test shape: (160165, 45)\n"
     ]
    }
   ],
   "source": [
    "# Dataset Preparation - UNSW-NB15\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Adjust path to find the UNSWNB15 package\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from UNSWNB15.preprocessing.dataset import UNSWNB15\n",
    "\n",
    "print(\"Loading UNSW-NB15 dataset (small size for testing)...\")\n",
    "\n",
    "# 1. Initialization\n",
    "dataset = UNSWNB15(dataset_size=\"small\")\n",
    "\n",
    "# 2. Optimization and Encoding\n",
    "dataset.optimize_memory()\n",
    "dataset.encode(attack_encoder=\"label\")\n",
    "\n",
    "# 3. Scaling (Normalization)\n",
    "try:\n",
    "    dataset.scale(scaler=\"minmax\")\n",
    "except:\n",
    "    print(\"Warning: 'minmax' scaler not supported, falling back to standard scaler.\")\n",
    "    dataset.scale(scaler=\"standard\")\n",
    "\n",
    "# 4. Split Train/Test\n",
    "X_train, X_test, y_train, y_test = dataset.split(\n",
    "    test_size=0.25, \n",
    "    multiclass=False # Using binary classification\n",
    ")\n",
    "\n",
    "# 5. Explicit conversion for ART/PyTorch compatibility\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "# Create a small subset for quick attack testing\n",
    "X_sub = X_test[:100]\n",
    "y_sub = y_test[:100]\n",
    "\n",
    "print(f\"UNSW-NB15 data ready. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d1b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation - CICIDS2017 (OVERWRITING UNSW-NB15 VARIABLES)\n",
    "# CAREFUL: This will overwrite the UNSW-NB15 dataset variables.\n",
    "# We have to run either this cell or the previous one for the test.\n",
    "\n",
    "from CICIDS2017.preprocessing.dataset import CICIDS2017\n",
    "\n",
    "print(\"\\n\\nLoading CICIDS2017 dataset...\")\n",
    "dataset_cicids = CICIDS2017(dataset_size=None) \n",
    "dataset_cicids.optimize_memory()\n",
    "dataset_cicids.encode(attack_encoder=\"label\")\n",
    "\n",
    "try:\n",
    "    dataset_cicids.scale(scaler=\"minmax\")\n",
    "except:\n",
    "    print(\"Warning: 'minmax' scaler not supported, falling back to standard scaler.\")\n",
    "    dataset_cicids.scale(scaler=\"standard\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = dataset_cicids.split(test_size=0.25, multiclass=False)\n",
    "\n",
    "X_sub = X_test[:100].astype(np.float32)\n",
    "y_sub = y_test[:100].astype(np.int64)\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.int64)\n",
    "\n",
    "print(f\"CICIDS2017 data loaded. Current Train shape: {X_train.shape}, Test subset shape: {X_sub.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7408d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Model trained.\n",
      "Baseline accuracy on subset: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Train Target Model (KNN)\n",
    "\n",
    "knn_model, _ = train_knn(X_train, y_train, n_neighbors=5)\n",
    "print(\"KNN Model trained.\")\n",
    "\n",
    "# Baseline accuracy on the subset\n",
    "baseline_acc = accuracy_score(y_sub, knn_model.predict(X_sub))\n",
    "print(f\"Baseline accuracy on subset: {baseline_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8c6847a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating adversarial examples using HopSkipJump (Black Box)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104a4e87a69c465dbdbce4f8e857cda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HopSkipJump:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- HopSkipJump Results (Black Box) ---\n",
      "Accuracy on clean data: 0.99\n",
      "Accuracy on adversarial data: 0.01\n",
      "Performance drop: 98.0%\n"
     ]
    }
   ],
   "source": [
    "# Black Box Attack - HopSkipJump\n",
    "# We do not know the model internals, only the output class.\n",
    "\n",
    "# 1. Wrap the Scikit-Learn model for ART\n",
    "art_knn = SklearnClassifier(model=knn_model, clip_values=(0.0, 1.0)) # Because our data is normalized\n",
    "\n",
    "# 2. Configure the HopSkipJump attack\n",
    "attack_hsj = HopSkipJump(classifier=art_knn, targeted=False, max_iter=50, max_eval=1000, init_eval=10)\n",
    "\n",
    "print(\"Generating adversarial examples using HopSkipJump (Black Box)...\")\n",
    "X_adv_hsj = attack_hsj.generate(x=X_sub)\n",
    "\n",
    "# 3. Evaluate the attack\n",
    "preds_clean = knn_model.predict(X_sub)\n",
    "preds_adv_hsj = knn_model.predict(X_adv_hsj)\n",
    "\n",
    "acc_clean = accuracy_score(y_sub, preds_clean)\n",
    "acc_adv = accuracy_score(y_sub, preds_adv_hsj)\n",
    "\n",
    "print(f\"\\n--- HopSkipJump Results (Black Box) ---\")\n",
    "print(f\"Accuracy on clean data: {acc_clean:.2f}\")\n",
    "print(f\"Accuracy on adversarial data: {acc_adv:.2f}\")\n",
    "print(f\"Performance drop: {(acc_clean - acc_adv)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8841c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surrogate PyTorch model initialized.\n"
     ]
    }
   ],
   "source": [
    "# White Box Strategy - Defining a PyTorch Surrogate\n",
    "# Since KNN is non-differentiable (no gradients), we cannot use the Fast Gradient Sign Method (FGSM) directly.\n",
    "# We will train a differentiable Neural Network (Surrogate) to mimic the KNN, attack the surrogate, and transfer the examples to the KNN.\n",
    "\n",
    "class SurrogateModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SurrogateModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "# We use KNN predictions (y_train_surrogate) as labels, not the ground truth.\n",
    "# We want the surrogate to learn the KNN's mistakes and decision boundaries.\n",
    "y_train_surrogate = knn_model.predict(X_train)\n",
    "\n",
    "tensor_x = torch.Tensor(X_train).to(device)\n",
    "tensor_y = torch.LongTensor(y_train_surrogate).to(device)\n",
    "\n",
    "dataset = TensorDataset(tensor_x, tensor_y)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "surrogate = SurrogateModel(input_size=X_train.shape[1], num_classes=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(surrogate.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Surrogate PyTorch model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d0f4e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training surrogate model to mimic KNN...\n",
      "Epoch [5/30], Loss: 0.0000\n",
      "Epoch [10/30], Loss: 0.0208\n",
      "Epoch [15/30], Loss: 0.0052\n",
      "Epoch [20/30], Loss: 0.0001\n",
      "Epoch [25/30], Loss: 0.0209\n",
      "Epoch [30/30], Loss: 0.0000\n",
      "Surrogate training complete.\n"
     ]
    }
   ],
   "source": [
    "# Train the Surrogate Model\n",
    "epochs = 30\n",
    "surrogate.train()\n",
    "\n",
    "print(\"Training surrogate model to mimic KNN...\")\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = surrogate(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Surrogate training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13103432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating adversarial examples using FGSM (on Surrogate)...\n",
      "\n",
      "--- Transfer Attack Results (White Box Surrogate) ---\n",
      "KNN Accuracy (Clean): 0.99\n",
      "KNN Accuracy (Transferred Attack): 0.99\n",
      "Attack Success (Drop): 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Transfer Attack (FGSM via Surrogate)\n",
    "# We generate attacks on the differentiable Surrogate using gradients,\n",
    "# then test if these attacks also fool the KNN.\n",
    "\n",
    "# 1. Wrap the PyTorch model for ART\n",
    "art_surrogate = PyTorchClassifier(\n",
    "    model=surrogate,\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(X_train.shape[1],),\n",
    "    nb_classes=2,\n",
    "    clip_values=(0.0, 1.0),\n",
    "    device_type='gpu' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# 2. Generate attacks using FGSM\n",
    "# eps = perturbation magnitude (0.1 is noticeable, 0.01 is subtle)\n",
    "attack_fgsm = FastGradientMethod(estimator=art_surrogate, eps=0.3)\n",
    "\n",
    "print(\"Generating adversarial examples using FGSM (on Surrogate)...\")\n",
    "X_adv_surr = attack_fgsm.generate(x=X_sub)\n",
    "\n",
    "# 3. Transfer Evaluation: Test the generated examples on the original KNN\n",
    "preds_clean_knn = knn_model.predict(X_sub)\n",
    "preds_adv_transfer = knn_model.predict(X_adv_surr)\n",
    "\n",
    "acc_clean = accuracy_score(y_sub, preds_clean_knn)\n",
    "acc_transf = accuracy_score(y_sub, preds_adv_transfer)\n",
    "\n",
    "print(f\"\\n--- Transfer Attack Results (White Box Surrogate) ---\")\n",
    "print(f\"KNN Accuracy (Clean): {acc_clean:.2f}\")\n",
    "print(f\"KNN Accuracy (Transferred Attack): {acc_transf:.2f}\")\n",
    "print(f\"Attack Success (Drop): {(acc_clean - acc_transf)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c5752",
   "metadata": {},
   "source": [
    "HopSkipJump\t98.0%\tVulnérable aux attaques basées sur les décisions.\n",
    "FGSM via Surrogate\t0.0%\tRobuste aux attaques basées sur les gradients (car la fonction de perte du KNN est trop différente de celle du Surrogate)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
