{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad655ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_dir = os.getcwd().split(\"AdversarialNIDS\")[0] + \"AdversarialNIDS\"\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from scripts.logger import LoggerManager\n",
    "from scripts.model_analyzer import perform_model_analysis\n",
    "\n",
    "from CICIDS2017.preprocessing.dataset import CICIDS2017\n",
    "from UNSWNB15.preprocessing.dataset import UNSWNB15\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6efa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 21:26:28,401 - INFO - Logger initialized\n",
      "2025-11-18 21:26:28,403 - INFO - Downloading dataset: sweety18/cicids2017-full-dataset\n",
      "2025-11-18 21:26:29,084 - INFO - Loading data\n",
      "2025-11-18 21:26:42,599 - INFO - Initial dimensions: 2,214,469 rows x 79 columns = 174,943,051 cells\n",
      "2025-11-18 21:26:57,557 - INFO - ============================================================\n",
      "2025-11-18 21:26:57,557 - INFO - Preprocessing completed successfully\n",
      "2025-11-18 21:26:57,558 - INFO - Final dimensions: 1,942,693 rows x 71 columns\n",
      "2025-11-18 21:26:57,558 - INFO - Total rows removed: 271,776 (12.27%)\n",
      "2025-11-18 21:26:57,559 - INFO - data retention rate: 87.73%\n",
      "2025-11-18 21:26:57,559 - INFO - ============================================================\n",
      "2025-11-18 21:26:57,561 - INFO - Encoding attack labels...\n",
      "2025-11-18 21:26:57,990 - INFO - Attack labels encoded using LabelEncoder() encoder.\n",
      "2025-11-18 21:26:57,991 - INFO - Scaling dataset features...\n",
      "2025-11-18 21:27:24,188 - INFO - Features scaled using MinMaxScaler() scaler.\n",
      "2025-11-18 21:27:24,199 - INFO - Optimizing memory usage of the dataset...\n",
      "2025-11-18 21:27:24,203 - INFO - Initial memory usage: 1067.15 MB\n",
      "2025-11-18 21:27:24,880 - INFO - Optimized memory usage: 555.81 MB\n",
      "2025-11-18 21:27:24,881 - INFO - Memory reduction: 511.34 MB (47.92%)\n"
     ]
    }
   ],
   "source": [
    "lm = LoggerManager(log_dir=f\"{root_dir}/logs\", log_name=\"test_dl_models\")\n",
    "lm.logger.info(\"Logger initialized\")\n",
    "\n",
    "dataset = UNSWNB15(logger=lm.logger).encode(attack_encoder=\"label\").scale(scaler=\"minmax\").optimize_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216296be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 21:27:24,887 - INFO - Splitting dataset into training and testing sets...\n",
      "2025-11-18 21:28:11,055 - INFO - Class distribution after SMOTE:\n",
      "2025-11-18 21:28:11,056 - INFO -   Class 0: 1222532 samples\n",
      "2025-11-18 21:28:11,056 - INFO -   Class 1: 1222532 samples\n",
      "2025-11-18 21:28:11,057 - INFO -   Class 2: 1222532 samples\n",
      "2025-11-18 21:28:11,057 - INFO -   Class 3: 1222532 samples\n",
      "2025-11-18 21:28:11,058 - INFO -   Class 4: 1222532 samples\n",
      "2025-11-18 21:28:11,058 - INFO -   Class 5: 1222532 samples\n",
      "2025-11-18 21:28:11,059 - INFO -   Class 6: 1222532 samples\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = dataset.split(\n",
    "    multiclass=True,\n",
    "    apply_smote=True,\n",
    "    oneHot=True,\n",
    "    toTensor=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e92fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nids_model(model, optimizer, scheduler, criterion, train_loader, val_loader, device, epochs=25):\n",
    "    epoch_losses = []\n",
    "    epoch_val_losses = []\n",
    "    # Training loop\n",
    "    tqdm_epochs = tqdm(range(int(epochs)), desc=\"Training Progress\")\n",
    "    for epoch in tqdm_epochs:\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for X_train, y_train in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(X_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            losses.append(loss)\n",
    "\n",
    "        epoch_loss = sum(losses) / len(losses)\n",
    "        epoch_losses.append(epoch_loss.cpu().detach().numpy())\n",
    "            \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step(epoch_loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                val_outputs = model(X_val)\n",
    "                val_loss = criterion(val_outputs, y_val)\n",
    "                val_losses.append(val_loss)\n",
    "\n",
    "            epoch_val_loss = sum(val_losses) / len(val_losses)\n",
    "            epoch_val_losses.append(epoch_val_loss.cpu().detach().numpy())\n",
    "            \n",
    "        tqdm_epochs.set_description(f\"Loss: {epoch_loss.item():.4f}, Val Loss: {epoch_val_loss.item():.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    return model, epoch_losses, epoch_val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7033660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_loss(list_epoch_loss, list_val_loss, title, dir, logger, epoch_min=2):\n",
    "    lm.logger.info(\"Plotting loss curve...\")\n",
    "    # Plotting loss curve with linear and log scale\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(list_epoch_loss[epoch_min:], label='Training Loss')\n",
    "    plt.plot(list_val_loss[epoch_min:], '-r', label='Validation Loss')\n",
    "    plt.title(f\"Loss Curve - {title}\")  \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(list_epoch_loss[epoch_min:], label='Training Loss')\n",
    "    plt.plot(list_val_loss[epoch_min:], '-r', label='Validation Loss') \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    loss_plot_path = f\"{dir}/loss_img/{title}_loss.png\"\n",
    "    os.makedirs(f\"{dir}/loss_img\", exist_ok=True)\n",
    "    plt.savefig(loss_plot_path, bbox_inches='tight', dpi=300)\n",
    "    lm.logger.info(f\"Loss curve saved as {loss_plot_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a885ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train.to(device), y_train.to(device))\n",
    "val_dataset = TensorDataset(X_val.to(device), y_val.to(device))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5fbad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 70, Num classes: 7\n"
     ]
    }
   ],
   "source": [
    "input_size = train_loader.dataset.tensors[0].shape[1]\n",
    "num_classes = train_loader.dataset.tensors[1].shape[1]\n",
    "print(f\"Input size: {input_size}, Num classes: {num_classes}\")  \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f70dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkIntrusionMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NetworkIntrusionMLP, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = self.classifier(features)\n",
    "        return torch.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf43053",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp = NetworkIntrusionMLP(input_size=input_size, num_classes=num_classes).to(device)\n",
    "\n",
    "learning_rate_mlp = 1e-2\n",
    "num_epochs_mlp = 50\n",
    "\n",
    "optimizer_mlp = optim.Adam(model_mlp.parameters(), lr=learning_rate_mlp)\n",
    "scheduler_mlp = optim.lr_scheduler.ReduceLROnPlateau(optimizer_mlp, mode='min', factor=0.5, patience=5, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53126928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/50 [00:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Of the allocated memory 7.46 GiB is allocated by PyTorch, and 15.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_mlp, train_losses_mlp, val_losses_mlp = \u001b[43mtrain_nids_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs_mlp\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_nids_model\u001b[39m\u001b[34m(model, optimizer, scheduler, criterion, train_loader, val_loader, device, epochs)\u001b[39m\n\u001b[32m      8\u001b[39m losses = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X_train, y_train \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     loss = criterion(outputs, y_train)\n\u001b[32m     13\u001b[39m     losses.append(loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cours/TelecomSE/FISE2/Semestre 7/Projet/AdversarialNIDS/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cours/TelecomSE/FISE2/Semestre 7/Projet/AdversarialNIDS/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mNetworkIntrusionMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.classifier(features)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.softmax(out, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cours/TelecomSE/FISE2/Semestre 7/Projet/AdversarialNIDS/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cours/TelecomSE/FISE2/Semestre 7/Projet/AdversarialNIDS/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cours/TelecomSE/FISE2/Semestre 7/Projet/AdversarialNIDS/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cours/TelecomSE/FISE2/Semestre 7/Projet/AdversarialNIDS/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cours/TelecomSE/FISE2/Semestre 7/Projet/AdversarialNIDS/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cours/TelecomSE/FISE2/Semestre 7/Projet/AdversarialNIDS/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Of the allocated memory 7.46 GiB is allocated by PyTorch, and 15.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model_mlp, train_losses_mlp, val_losses_mlp = train_nids_model(\n",
    "    model=model_mlp,\n",
    "    optimizer=optimizer_mlp,\n",
    "    scheduler=scheduler_mlp,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    epochs=num_epochs_mlp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_loss(\n",
    "    list_epoch_loss=train_losses_mlp,\n",
    "    list_val_loss=val_losses_mlp,\n",
    "    title=\"MLP_NIDS\",\n",
    "    dir=root_dir,\n",
    "    logger=lm.logger,\n",
    "    epoch_min=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad28e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract X and y from the validation dataset\n",
    "#X_val = torch.stack([val_dataset[i][0] for i in range(len(val_dataset))]).to(device)\n",
    "#y_val = torch.stack([val_dataset[i][1] for i in range(len(val_dataset))]).to(device)\n",
    "\n",
    "perform_model_analysis(\n",
    "    model=model_mlp,\n",
    "    X_test=X_val,\n",
    "    y_test=y_val,\n",
    "    logger=lm.logger,\n",
    "    model_name=\"MLP_NIDS\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35635dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkIntrustionCNN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(NetworkIntrustionCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # 1D Convolutional Layers\n",
    "            nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * (input_size // 4), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input for 1D convolution\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        features = self.features(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        out = self.classifier(features)\n",
    "        return torch.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f2afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = NetworkIntrustionCNN(input_channels=1, num_classes=num_classes).to(device)\n",
    "\n",
    "learning_rate_cnn = 1e-2\n",
    "num_epochs_cnn = 100\n",
    "\n",
    "optimizer_cnn = optim.Adam(model_cnn.parameters(), lr=learning_rate_cnn)\n",
    "scheduler_cnn = optim.lr_scheduler.PolynomialLR(optimizer_cnn, total_iters=num_epochs_cnn, power=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn, train_loss_cnn, val_loss_cnn = train_nids_model(\n",
    "    model=model_cnn,\n",
    "    optimizer=optimizer_cnn,\n",
    "    scheduler=scheduler_cnn,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    epochs=num_epochs_cnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a447ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_loss(\n",
    "    train_loss_cnn,\n",
    "    val_loss_cnn,\n",
    "    title=\"NetworkIntrustionCNN\",\n",
    "    dir=root_dir,\n",
    "    logger=lm.logger,\n",
    "    epoch_min=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d23420",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_model_analysis(\n",
    "    model=model_cnn\n",
    "    X_test=X,\n",
    "    y_test=y,\n",
    "    logger=lm.logger,\n",
    "    model_name=\"MLP_NIDS\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkIntrusionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(NetworkIntrusionLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM expects (batch, seq_len, features)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use the last time step\n",
    "        out = lstm_out\n",
    "        out = self.classifier(out)\n",
    "        return torch.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = NetworkIntrusionLSTM(input_size=input_size, hidden_size=128, num_layers=3, num_classes=num_classes).to(device)\n",
    "\n",
    "learning_rate_lstm = 1e-2\n",
    "num_epochs_lstm = 100\n",
    "\n",
    "optimizer_lstm = optim.Adam(model_lstm.parameters(), lr=learning_rate_lstm)\n",
    "scheduler_lstm = optim.lr_scheduler.PolynomialLR(optimizer_lstm, total_iters=num_epochs_lstm, power=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm, train_loss_lstm, val_loss_lstm = train_nids_model(\n",
    "    model=model_lstm,\n",
    "    optimizer=optimizer_lstm,\n",
    "    scheduler=scheduler_lstm,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    epochs=num_epochs_lstm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_loss(\n",
    "    train_loss_lstm, \n",
    "    val_loss_lstm, \n",
    "    title=\"LSTM_NIDS_Model\", \n",
    "    dir=root_dir, \n",
    "    logger=lm.logger,\n",
    "    epoch_min=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
