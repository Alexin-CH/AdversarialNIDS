{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d412408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(current_dir, \"..\"))\n",
    "\n",
    "from scripts.logger import LoggerManager\n",
    "from scripts.generaldataset import UNSWNB15\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9926f92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 13:55:42,646 - INFO - Logger initialized\n",
      "2025-11-17 13:55:42,648 - INFO - Downloading dataset: mrwellsdavid/unsw-nb15\n",
      "2025-11-17 13:55:43,511 - INFO - Loading data\n",
      "c:\\msys64\\home\\valen\\TDpython\\AdversarialNIDS\\UNSW-NB15\\..\\scripts\\generaldownload.py:54: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[\"Attack Type\"].fillna(\"Benign\", inplace=True)\n",
      "2025-11-17 13:55:49,166 - INFO - Initial dimensions: 700,000 rows x 46 columns = 32,200,000 cells\n",
      "2025-11-17 13:55:53,857 - INFO - ============================================================\n",
      "2025-11-17 13:55:53,859 - INFO - Preprocessing completed successfully\n",
      "2025-11-17 13:55:53,862 - INFO - Final dimensions: 640,658 rows x 46 columns\n",
      "2025-11-17 13:55:53,864 - INFO - Total rows removed: 59,342 (8.48%)\n",
      "2025-11-17 13:55:53,865 - INFO - data retention rate: 91.52%\n",
      "2025-11-17 13:55:53,866 - INFO - ============================================================\n",
      "2025-11-17 13:55:53,871 - INFO - Encoding attack labels...\n",
      "2025-11-17 13:55:55,377 - INFO - Attack labels encoded using OneHotEncoder(sparse_output=False) encoder.\n",
      "2025-11-17 13:55:55,381 - INFO - Scaling dataset features...\n",
      "2025-11-17 13:55:56,381 - INFO - Features scaled using MinMaxScaler() scaler.\n",
      "2025-11-17 13:55:56,424 - INFO - Optimizing memory usage of the dataset...\n",
      "2025-11-17 13:55:56,435 - INFO - Initial memory usage: 229.73 MB\n",
      "2025-11-17 13:55:56,836 - INFO - Optimized memory usage: 124.64 MB\n",
      "2025-11-17 13:55:56,837 - INFO - Memory reduction: 105.09 MB (45.74%)\n"
     ]
    }
   ],
   "source": [
    "lm = LoggerManager(log_dir=f\"{current_dir}/logs\", log_name=\"test_dl_models\")\n",
    "lm.logger.info(\"Logger initialized\")\n",
    "\n",
    "dataset = UNSWNB15(logger=lm.logger).encode(attack_encoder=\"onehot\").scale(scaler=\"minmax\").optimize_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caffb21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCELoss()\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_nids_model(model, optimizer, scheduler, train_loader, val_loader, device, epochs=25):\n",
    "    epoch_losses = []\n",
    "    epoch_val_losses = []\n",
    "    # Training loop\n",
    "    tqdm_epochs = tqdm(range(int(epochs)), desc=\"Training Progress\")\n",
    "    for epoch in tqdm_epochs:\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for X_train, y1_train, y2_train in train_loader:\n",
    "            # Forward pass\n",
    "            out1, out2 = model(X_train)\n",
    "            loss1 = bce_loss(out1, y1_train)\n",
    "            loss2 = ce_loss(out2, y2_train)\n",
    "            loss = loss1 + loss2\n",
    "            losses.append(loss)\n",
    "\n",
    "        epoch_loss = sum(losses) / len(losses)\n",
    "        epoch_losses.append(epoch_loss.cpu().detach().numpy())\n",
    "            \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step(epoch_loss.detach())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for X_val, y1_val, y2_val in val_loader:\n",
    "                val_out1, val_out2 = model(X_val)\n",
    "                val_loss = bce_loss(val_out1, y1_val) + ce_loss(val_out2, y2_val)\n",
    "                val_losses.append(val_loss)\n",
    "\n",
    "            epoch_val_loss = sum(val_losses) / len(val_losses)\n",
    "            epoch_val_losses.append(epoch_val_loss.cpu().detach().numpy())\n",
    "            \n",
    "        tqdm_epochs.set_description(f\"Loss: {epoch_loss.item():.4f}, Val Loss: {epoch_val_loss.item():.4f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "    return model, epoch_losses, epoch_val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aeecda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_loss(list_epoch_loss, list_val_loss, title, dir, logger, epoch_min=2):\n",
    "    lm.logger.info(\"Plotting loss curve...\")\n",
    "    # Plotting loss curve with linear and log scale\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(list_epoch_loss[epoch_min:], label='Training Loss')\n",
    "    plt.plot(list_val_loss[epoch_min:], '-r', label='Validation Loss')\n",
    "    plt.title(f\"Loss Curve - {title}\")  \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(list_epoch_loss[epoch_min:], label='Training Loss')\n",
    "    plt.plot(list_val_loss[epoch_min:], '-r', label='Validation Loss') \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    loss_plot_path = f\"{dir}/loss_img/{title}_loss.png\"\n",
    "    os.makedirs(f\"{dir}/loss_img\", exist_ok=True)\n",
    "    plt.savefig(loss_plot_path, bbox_inches='tight', dpi=300)\n",
    "    lm.logger.info(f\"Loss curve saved as {loss_plot_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e739a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(dataset.scaled_features).to(device)\n",
    "y1 = torch.FloatTensor(dataset.is_attack).to(device)\n",
    "y2 = torch.FloatTensor(dataset.attack_classes).to(device)\n",
    "\n",
    "dataset_size = len(X)\n",
    "\n",
    "# Randomly get a subset of the data\n",
    "subset_size = min(50000, dataset_size)\n",
    "indices = torch.randperm(dataset_size)[:subset_size]\n",
    "X = X[indices]\n",
    "y1 = y1[indices]\n",
    "y2 = y2[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e5f586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 48958 samples\n",
      "Class 1: 1042 samples\n",
      "Class 'benign' (0): 0 samples\n"
     ]
    }
   ],
   "source": [
    "# Display num of elements per class\n",
    "unique, counts = torch.unique(torch.argmax(y1, dim=1), return_counts=True)\n",
    "class_distribution = dict(zip(unique.cpu().numpy(), counts.cpu().numpy()))\n",
    "for cls, count in class_distribution.items():\n",
    "    print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "difference = subset_size - sum(counts).item()\n",
    "print(f\"Class 'benign' (0): {difference} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec556f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 281 samples\n",
      "Class 1: 16 samples\n",
      "Class 2: 30 samples\n",
      "Class 3: 48958 samples\n",
      "Class 4: 65 samples\n",
      "Class 5: 293 samples\n",
      "Class 6: 232 samples\n",
      "Class 7: 112 samples\n",
      "Class 8: 10 samples\n",
      "Class 9: 3 samples\n"
     ]
    }
   ],
   "source": [
    "# Display num of elements per class\n",
    "unique, counts = torch.unique(torch.argmax(y2, dim=1), return_counts=True)\n",
    "class_distribution = dict(zip(unique.cpu().numpy(), counts.cpu().numpy()))\n",
    "for cls, count in class_distribution.items():\n",
    "    print(f\"Class {cls}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f80e941a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 50000, Train size: 40000, Val size: 10000\n"
     ]
    }
   ],
   "source": [
    "dataset_tensor = TensorDataset(X, y1, y2)\n",
    "\n",
    "train_size = int(0.8 * len(dataset_tensor))\n",
    "val_size = len(dataset_tensor) - train_size\n",
    "\n",
    "print(f\"Dataset size: {len(dataset_tensor)}, Train size: {train_size}, Val size: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7d59945",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset_tensor, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfa49017",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "680bd867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 45, Num classes: 10\n"
     ]
    }
   ],
   "source": [
    "input_size = dataset.scaled_features.shape[1]\n",
    "num_classes = len(dataset.attack_classes[0])\n",
    "print(f\"Input size: {input_size}, Num classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2c52aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkIntrusionMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NetworkIntrusionMLP, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.RReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.classifier1 = nn.Sequential(\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes[0]),\n",
    "        )\n",
    "\n",
    "        self.classifier2 = nn.Sequential(\n",
    "            nn.Linear(64 + num_classes[0], 64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes[1]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out1 = self.classifier1(features)\n",
    "        # Combine features and out1 for second classifier\n",
    "        x2 = torch.cat((features, out1), dim=1)\n",
    "        out2 = self.classifier2(x2)\n",
    "        return torch.softmax(out1, dim=1), torch.softmax(out2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21ba1f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in MLP model: 24828\n"
     ]
    }
   ],
   "source": [
    "model_mlp = NetworkIntrusionMLP(input_size=input_size, num_classes=[2, num_classes]).to(device)\n",
    "num_parameters = sum(p.numel() for p in model_mlp.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters in MLP model: {num_parameters}\")\n",
    "\n",
    "learning_rate_mlp = 1e-2\n",
    "num_epochs_mlp = 1000\n",
    "\n",
    "optimizer_mlp = optim.AdamW(model_mlp.parameters(), lr=learning_rate_mlp)\n",
    "scheduler_mlp = optim.lr_scheduler.ReduceLROnPlateau(optimizer_mlp, mode='min', factor=0.8, patience=8, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd2b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.5029, Val Loss: 1.4987, LR: 1.00e-02:   3%|â–Ž         | 32/1000 [01:42<51:52,  3.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_mlp, train_losses_mlp, val_losses_mlp = \u001b[43mtrain_nids_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs_mlp\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtrain_nids_model\u001b[39m\u001b[34m(model, optimizer, scheduler, train_loader, val_loader, device, epochs)\u001b[39m\n\u001b[32m     33\u001b[39m val_losses = []\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X_val, y1_val, y2_val \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     val_out1, val_out2 = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     val_loss = bce_loss(val_out1, y1_val) + ce_loss(val_out2, y2_val)\n\u001b[32m     37\u001b[39m     val_losses.append(val_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\valen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\valen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mNetworkIntrusionMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     33\u001b[39m out1 = \u001b[38;5;28mself\u001b[39m.classifier1(features)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Combine features and out1 for second classifier\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m x2 = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m out2 = \u001b[38;5;28mself\u001b[39m.classifier2(x2)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.softmax(out1, dim=\u001b[32m1\u001b[39m), torch.softmax(out2, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_mlp, train_losses_mlp, val_losses_mlp = train_nids_model(\n",
    "    model=model_mlp,\n",
    "    optimizer=optimizer_mlp,\n",
    "    scheduler=scheduler_mlp,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    epochs=num_epochs_mlp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_loss(\n",
    "    list_epoch_loss=train_losses_mlp,\n",
    "    list_val_loss=val_losses_mlp,\n",
    "    title=\"MLP_NIDS\",\n",
    "    dir=current_dir,\n",
    "    logger=lm.logger,\n",
    "    epoch_min=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
