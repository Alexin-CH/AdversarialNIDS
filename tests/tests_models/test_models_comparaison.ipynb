{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "621a8d2a",
   "metadata": {},
   "source": [
    "# Complete Model Comparison on CICIDS2017 Dataset\n",
    "\n",
    "This notebook trains and compares **Random Forest**, **KNN**, and **Decision Tree** classifiers on the CICIDS2017 intrusion detection dataset.\n",
    "\n",
    "## Models Tested:\n",
    "1. **Random Forest** - Ensemble of decision trees\n",
    "2. **K-Nearest Neighbors (KNN)** - Instance-based learning\n",
    "3. **Decision Tree** - Single tree classifier\n",
    "\n",
    "## Features:\n",
    "- Unified preprocessing pipeline\n",
    "- SMOTE for class balancing\n",
    "- Cross-validation for all models\n",
    "- Comprehensive performance comparison\n",
    "- Feature importance analysis\n",
    "- Visual comparison of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ba440",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "\n",
    "# Import dataset\n",
    "from CICIDS2017.preprocessing.dataset import CICIDS2017\n",
    "\n",
    "# Import shared utilities\n",
    "from scripts.models.model_utils import (\n",
    "    prepare_data,\n",
    "    evaluate_model,\n",
    "    check_data_leakage,\n",
    "    get_feature_importance,\n",
    "    balance_classes_info,\n",
    "    remove_rare_classes,\n",
    "    print_performance_summary\n",
    ")\n",
    "\n",
    "# Import model-specific modules\n",
    "from scripts.models.random_forest import create_rf_pipeline, train_random_forest\n",
    "from scripts.models.knn import create_knn_pipeline, train_knn, find_optimal_k\n",
    "from scripts.models.decision_tree import (\n",
    "    create_dt_pipeline, \n",
    "    train_decision_tree,\n",
    "    analyze_tree_complexity\n",
    ")\n",
    "\n",
    "# Import sklearn utilities\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import logger\n",
    "from scripts.logger import LoggerManager\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e1626",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'sample_size': 100000,  # Adjust based on your resources\n",
    "    'test_size': 0.25,\n",
    "    'cv_folds': 5,\n",
    "    'random_state': 0,\n",
    "    'use_smote': True,\n",
    "    'variance_threshold': 0.01,\n",
    "    'leakage_features': ['Attack Number']  # Known leakage features\n",
    "}\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    'random_forest': {\n",
    "        'n_estimators': 10,\n",
    "        'max_depth': 3,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 2,\n",
    "        'max_features': 'sqrt',\n",
    "        'class_weight': 'balanced'\n",
    "    },\n",
    "    'knn': {\n",
    "        'n_neighbors': 5,  # Will be optimized\n",
    "        'weights': 'distance',\n",
    "        'metric': 'minkowski',\n",
    "        'p': 2\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'max_depth': 3,\n",
    "        'min_samples_split': 10,\n",
    "        'min_samples_leaf': 5,\n",
    "        'criterion': 'gini',\n",
    "        'class_weight': 'balanced'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Sample size: {CONFIG['sample_size']:,}\")\n",
    "print(f\"  Test size: {CONFIG['test_size']}\")\n",
    "print(f\"  CV folds: {CONFIG['cv_folds']}\")\n",
    "print(f\"  SMOTE: {CONFIG['use_smote']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f800a652",
   "metadata": {},
   "source": [
    "## 3. Initialize Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = LoggerManager(log_name=\"model_comparison\").get_logger()\n",
    "logger.info(\"Starting complete model comparison notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f2ab0",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "logger.info(\"Loading CICIDS2017 dataset...\")\n",
    "dataset = CICIDS2017(logger=logger)\n",
    "dataset.encode().optimize_memory()\n",
    "data = dataset.data\n",
    "\n",
    "print(f\"Original dataset shape: {data.shape}\")\n",
    "print(f\"Columns: {len(data.columns)}\")\n",
    "print(f\"Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffbef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "logger.info(f\"Sampling {CONFIG['sample_size']} rows...\")\n",
    "data_sample = data.sample(n=min(CONFIG['sample_size'], len(data)), \n",
    "                          random_state=CONFIG['random_state'])\n",
    "\n",
    "print(f\"Sampled data shape: {data_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data using shared utility\n",
    "X, y, removed_features = prepare_data(\n",
    "    data_sample,\n",
    "    target_column='Attack Type',\n",
    "    leakage_features=CONFIG['leakage_features'],\n",
    "    remove_low_var=True,\n",
    "    var_threshold=CONFIG['variance_threshold'],\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "print(f\"\\nRemoved features:\")\n",
    "if removed_features is not None:\n",
    "    leakage = removed_features.get('leakage', [])\n",
    "    low_var = removed_features.get('low_variance', [])\n",
    "    print(f\"  Leakage: {len(leakage)}\")\n",
    "    if leakage:\n",
    "        print(f\"    Names: {list(leakage)}\")\n",
    "    print(f\"  Low variance: {len(low_var)}\")\n",
    "    if low_var:\n",
    "        print(f\"    Names: {list(low_var)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5189ab8",
   "metadata": {},
   "source": [
    "## 5. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "balance_info = balance_classes_info(y, logger=logger)\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "y.value_counts().plot(kind='bar', color='steelblue', edgecolor='black')\n",
    "plt.title('Class Distribution Before SMOTE', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Attack Type', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efa7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data leakage diagnostics\n",
    "diagnostics = check_data_leakage(X, y, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86995330",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a752d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare classes for stratified split\n",
    "X, y, removed_classes = remove_rare_classes(X, y, min_samples=2, logger=logger)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=CONFIG['test_size'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7de85",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation\n",
    "\n",
    "We'll train all three models and collect their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2384decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for results\n",
    "results = {}\n",
    "training_times = {}\n",
    "prediction_times = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744876a",
   "metadata": {},
   "source": [
    "### 7.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1efd9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TRAINING RANDOM FOREST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create pipeline\n",
    "rf_pipeline = create_rf_pipeline(\n",
    "    **MODEL_CONFIGS['random_forest'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    use_smote=CONFIG['use_smote'],\n",
    "    use_scaler=True\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "logger.info(\"Random Forest: Cross-validation...\")\n",
    "start_time = time()\n",
    "rf_cv_scores = cross_val_score(rf_pipeline, X_train, y_train, \n",
    "                                cv=CONFIG['cv_folds'], n_jobs=-1)\n",
    "cv_time = time() - start_time\n",
    "\n",
    "print(f\"CV Time: {cv_time:.2f}s\")\n",
    "print(f\"CV Scores: {rf_cv_scores}\")\n",
    "print(f\"Mean CV: {rf_cv_scores.mean():.4f} (+/- {rf_cv_scores.std():.4f})\")\n",
    "\n",
    "# Train final model\n",
    "logger.info(\"Random Forest: Training final model...\")\n",
    "start_time = time()\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "training_times['Random Forest'] = time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "start_time = time()\n",
    "rf_results = evaluate_model(rf_pipeline, X_test, y_test, logger=logger)\n",
    "prediction_times['Random Forest'] = time() - start_time\n",
    "\n",
    "results['Random Forest'] = {\n",
    "    'cv_scores': rf_cv_scores,\n",
    "    'test_accuracy': rf_results['accuracy'],\n",
    "    'report': rf_results['report'],\n",
    "    'confusion_matrix': rf_results['confusion_matrix'],\n",
    "    'model': rf_pipeline\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úì Random Forest completed\")\n",
    "print(f\"  Training time: {training_times['Random Forest']:.2f}s\")\n",
    "print(f\"  Test accuracy: {rf_results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52443778",
   "metadata": {},
   "source": [
    "### 7.2 K-Nearest Neighbors\n",
    "Why we use pipelines instead of the `train_knn` function\n",
    "\n",
    "In this notebook, we use scikit-learn pipelines for all models, including KNN, instead of the standalone `train_knn` function. Pipelines allow us to chain preprocessing steps (such as scaling and SMOTE) together with the model, ensuring that all transformations are applied consistently and only to the training data during cross-validation. This prevents data leakage and makes the workflow more robust and reproducible. The `train_knn` function does not integrate preprocessing or handle cross-validation in the same way, so using pipelines is considered best practice for reliable model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e619ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING K-NEAREST NEIGHBORS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Scale data for k-finding\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Find optimal k\n",
    "logger.info(\"KNN: Finding optimal k...\")\n",
    "k_results = find_optimal_k(\n",
    "    X_train_scaled, \n",
    "    y_train, \n",
    "    k_range=range(3, 16, 2),\n",
    "    cv=CONFIG['cv_folds'],\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "optimal_k = k_results['optimal_k']\n",
    "print(f\"\\nOptimal k: {optimal_k}\")\n",
    "\n",
    "# Create pipeline with optimal k\n",
    "knn_pipeline = create_knn_pipeline(\n",
    "    n_neighbors=optimal_k,\n",
    "    **{k: v for k, v in MODEL_CONFIGS['knn'].items() if k != 'n_neighbors'},\n",
    "    random_state=CONFIG['random_state'],\n",
    "    use_smote=CONFIG['use_smote'],\n",
    "    use_scaler=True  # CRITICAL for KNN\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "logger.info(\"KNN: Cross-validation...\")\n",
    "start_time = time()\n",
    "knn_cv_scores = cross_val_score(knn_pipeline, X_train, y_train, \n",
    "                                 cv=CONFIG['cv_folds'], n_jobs=-1)\n",
    "cv_time = time() - start_time\n",
    "\n",
    "print(f\"CV Time: {cv_time:.2f}s\")\n",
    "print(f\"CV Scores: {knn_cv_scores}\")\n",
    "print(f\"Mean CV: {knn_cv_scores.mean():.4f} (+/- {knn_cv_scores.std():.4f})\")\n",
    "\n",
    "# Train final model\n",
    "logger.info(\"KNN: Training final model...\")\n",
    "start_time = time()\n",
    "knn_pipeline.fit(X_train, y_train)\n",
    "training_times['KNN'] = time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "start_time = time()\n",
    "knn_results = evaluate_model(knn_pipeline, X_test, y_test, logger=logger)\n",
    "prediction_times['KNN'] = time() - start_time\n",
    "\n",
    "results['KNN'] = {\n",
    "    'cv_scores': knn_cv_scores,\n",
    "    'test_accuracy': knn_results['accuracy'],\n",
    "    'report': knn_results['report'],\n",
    "    'confusion_matrix': knn_results['confusion_matrix'],\n",
    "    'model': knn_pipeline,\n",
    "    'optimal_k': optimal_k\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úì KNN completed\")\n",
    "print(f\"  Training time: {training_times['KNN']:.2f}s\")\n",
    "print(f\"  Test accuracy: {knn_results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c8ced",
   "metadata": {},
   "source": [
    "### 7.3 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ded89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING DECISION TREE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create pipeline\n",
    "dt_pipeline = create_dt_pipeline(\n",
    "    **MODEL_CONFIGS['decision_tree'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    use_smote=CONFIG['use_smote'],\n",
    "    use_scaler=False  # Not needed for DT\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "logger.info(\"Decision Tree: Cross-validation...\")\n",
    "start_time = time()\n",
    "dt_cv_scores = cross_val_score(dt_pipeline, X_train, y_train, \n",
    "                                cv=CONFIG['cv_folds'], n_jobs=-1)\n",
    "cv_time = time() - start_time\n",
    "\n",
    "print(f\"CV Time: {cv_time:.2f}s\")\n",
    "print(f\"CV Scores: {dt_cv_scores}\")\n",
    "print(f\"Mean CV: {dt_cv_scores.mean():.4f} (+/- {dt_cv_scores.std():.4f})\")\n",
    "\n",
    "# Train final model\n",
    "logger.info(\"Decision Tree: Training final model...\")\n",
    "start_time = time()\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "training_times['Decision Tree'] = time() - start_time\n",
    "\n",
    "# Analyze tree complexity\n",
    "tree_complexity = analyze_tree_complexity(dt_pipeline, logger=logger)\n",
    "\n",
    "# Evaluate\n",
    "start_time = time()\n",
    "dt_results = evaluate_model(dt_pipeline, X_test, y_test, logger=logger)\n",
    "prediction_times['Decision Tree'] = time() - start_time\n",
    "\n",
    "results['Decision Tree'] = {\n",
    "    'cv_scores': dt_cv_scores,\n",
    "    'test_accuracy': dt_results['accuracy'],\n",
    "    'report': dt_results['report'],\n",
    "    'confusion_matrix': dt_results['confusion_matrix'],\n",
    "    'model': dt_pipeline,\n",
    "    'tree_complexity': tree_complexity\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úì Decision Tree completed\")\n",
    "print(f\"  Training time: {training_times['Decision Tree']:.2f}s\")\n",
    "print(f\"  Test accuracy: {dt_results['accuracy']:.4f}\")\n",
    "print(f\"  Tree nodes: {tree_complexity['n_nodes']}\")\n",
    "print(f\"  Tree depth: {tree_complexity['max_depth']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b8d44",
   "metadata": {},
   "source": [
    "## 8. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe01e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name in ['Random Forest', 'KNN', 'Decision Tree']:\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Mean CV Score': results[model_name]['cv_scores'].mean(),\n",
    "        'CV Std': results[model_name]['cv_scores'].std(),\n",
    "        'Test Accuracy': results[model_name]['test_accuracy'],\n",
    "        'Training Time (s)': training_times[model_name],\n",
    "        'Prediction Time (s)': prediction_times[model_name]\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df['CV-Test Gap'] = abs(comparison_df['Mean CV Score'] - comparison_df['Test Accuracy'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model = comparison_df.loc[comparison_df['Test Accuracy'].idxmax(), 'Model']\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"   Test Accuracy: {comparison_df.loc[comparison_df['Test Accuracy'].idxmax(), 'Test Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e261eb",
   "metadata": {},
   "source": [
    "## 9. Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b99ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Accuracy Comparison\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "ax1.bar(x_pos - 0.2, comparison_df['Mean CV Score'], 0.4, \n",
    "        label='CV Score', color='steelblue', alpha=0.8)\n",
    "ax1.bar(x_pos + 0.2, comparison_df['Test Accuracy'], 0.4, \n",
    "        label='Test Accuracy', color='coral', alpha=0.8)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(comparison_df['Model'])\n",
    "ax1.set_ylabel('Accuracy', fontsize=11)\n",
    "ax1.set_title('Model Accuracy Comparison', fontsize=13, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim([0.8, 1.0])\n",
    "\n",
    "# 2. Training Time Comparison\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(comparison_df['Model'], comparison_df['Training Time (s)'], \n",
    "        color='lightgreen', edgecolor='black')\n",
    "ax2.set_ylabel('Time (seconds)', fontsize=11)\n",
    "ax2.set_title('Training Time Comparison', fontsize=13, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. CV Score Distribution\n",
    "ax3 = axes[1, 0]\n",
    "cv_data = [results[model]['cv_scores'] for model in ['Random Forest', 'KNN', 'Decision Tree']]\n",
    "ax3.boxplot(cv_data, labels=['RF', 'KNN', 'DT'])\n",
    "ax3.set_ylabel('CV Accuracy', fontsize=11)\n",
    "ax3.set_title('Cross-Validation Score Distribution', fontsize=13, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. CV-Test Gap\n",
    "ax4 = axes[1, 1]\n",
    "colors = ['green' if gap < 0.05 else 'orange' for gap in comparison_df['CV-Test Gap']]\n",
    "ax4.bar(comparison_df['Model'], comparison_df['CV-Test Gap'], color=colors, edgecolor='black')\n",
    "ax4.axhline(y=0.05, color='red', linestyle='--', label='Threshold (0.05)')\n",
    "ax4.set_ylabel('Gap', fontsize=11)\n",
    "ax4.set_title('CV-Test Gap (Overfitting Check)', fontsize=13, fontweight='bold')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208e96b",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7badda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, model_name in enumerate(['Random Forest', 'KNN', 'Decision Tree']):\n",
    "    cm = results[model_name]['confusion_matrix']\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                   display_labels=results[model_name]['model'].classes_)\n",
    "    disp.plot(ax=axes[idx], cmap='Blues', xticks_rotation=45)\n",
    "    axes[idx].set_title(f'{model_name}\\n(Acc: {results[model_name][\"test_accuracy\"]:.4f})', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cdf480",
   "metadata": {},
   "source": [
    "## 11. Feature Importance (Tree-based Models)\n",
    "KNN does not provide built-in feature importances because it makes predictions based on distances in feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importance for tree-based models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "rf_features = get_feature_importance(\n",
    "    results['Random Forest']['model'],\n",
    "    feature_names=list(X.columns),\n",
    "    top_n=15\n",
    ")\n",
    "features, importances = zip(*rf_features)\n",
    "axes[0].barh(range(len(features)), importances, color='steelblue')\n",
    "axes[0].set_yticks(range(len(features)))\n",
    "axes[0].set_yticklabels(features)\n",
    "axes[0].set_xlabel('Importance', fontsize=11)\n",
    "axes[0].set_title('Random Forest - Top 15 Features', fontsize=13, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Decision Tree Feature Importance\n",
    "dt_features = get_feature_importance(\n",
    "    results['Decision Tree']['model'],\n",
    "    feature_names=list(X.columns),\n",
    "    top_n=15\n",
    ")\n",
    "features, importances = zip(*dt_features)\n",
    "axes[1].barh(range(len(features)), importances, color='lightgreen')\n",
    "axes[1].set_yticks(range(len(features)))\n",
    "axes[1].set_yticklabels(features)\n",
    "axes[1].set_xlabel('Importance', fontsize=11)\n",
    "axes[1].set_title('Decision Tree - Top 15 Features', fontsize=13, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: KNN does not provide feature importance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Performance Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed summaries for each model\n",
    "for model_name in ['Random Forest', 'KNN', 'Decision Tree']:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"{model_name.upper()} DETAILED SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model_results = results[model_name]\n",
    "    \n",
    "    print(f\"\\nCross-Validation:\")\n",
    "    print(f\"  Scores: {model_results['cv_scores']}\")\n",
    "    print(f\"  Mean: {model_results['cv_scores'].mean():.4f}\")\n",
    "    print(f\"  Std: {model_results['cv_scores'].std():.4f}\")\n",
    "    \n",
    "    print(f\"\\nTest Set:\")\n",
    "    print(f\"  Accuracy: {model_results['test_accuracy']:.4f}\")\n",
    "    print(f\"  CV-Test Gap: {abs(model_results['cv_scores'].mean() - model_results['test_accuracy']):.4f}\")\n",
    "    \n",
    "    print(f\"\\nTiming:\")\n",
    "    print(f\"  Training: {training_times[model_name]:.2f}s\")\n",
    "    print(f\"  Prediction: {prediction_times[model_name]:.2f}s\")\n",
    "    print(f\"  Time per sample: {prediction_times[model_name]/len(X_test)*1000:.3f}ms\")\n",
    "    \n",
    "    # Model-specific info\n",
    "    if model_name == 'KNN':\n",
    "        print(f\"\\nKNN Specific:\")\n",
    "        print(f\"  Optimal k: {model_results['optimal_k']}\")\n",
    "        print(f\"  Memory usage: ~{X_train.memory_usage(deep=True).sum() / 1024**2:.2f} MB (stores all training data)\")\n",
    "    elif model_name == 'Decision Tree':\n",
    "        print(f\"\\nTree Complexity:\")\n",
    "        print(f\"  Total nodes: {model_results['tree_complexity']['n_nodes']}\")\n",
    "        print(f\"  Leaf nodes: {model_results['tree_complexity']['n_leaves']}\")\n",
    "        print(f\"  Max depth: {model_results['tree_complexity']['max_depth']}\")\n",
    "        print(f\"  Features used: {model_results['tree_complexity']['n_features_used']}/{X.shape[1]}\")\n",
    "    elif model_name == 'Random Forest':\n",
    "        print(f\"\\nRandom Forest Specific:\")\n",
    "        print(f\"  Number of trees: {MODEL_CONFIGS['random_forest']['n_estimators']}\")\n",
    "        print(f\"  Max depth per tree: {MODEL_CONFIGS['random_forest']['max_depth']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Determine best model by accuracy\n",
    "best_by_accuracy = comparison_df.loc[comparison_df['Test Accuracy'].idxmax()]\n",
    "best_by_speed = comparison_df.loc[comparison_df['Prediction Time (s)'].idxmin()]\n",
    "best_by_training = comparison_df.loc[comparison_df['Training Time (s)'].idxmin()]\n",
    "\n",
    "print(f\"\\nüèÜ Best Overall Accuracy:\")\n",
    "print(f\"   Model: {best_by_accuracy['Model']}\")\n",
    "print(f\"   Accuracy: {best_by_accuracy['Test Accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚ö° Fastest Prediction:\")\n",
    "print(f\"   Model: {best_by_speed['Model']}\")\n",
    "print(f\"   Time: {best_by_speed['Prediction Time (s)']:.2f}s\")\n",
    "\n",
    "print(f\"\\nüöÄ Fastest Training:\")\n",
    "print(f\"   Model: {best_by_training['Model']}\")\n",
    "print(f\"   Time: {best_by_training['Training Time (s)']:.2f}s\")\n",
    "\n",
    "print(\"\\nüìä Model Characteristics:\")\n",
    "print(\"\\n  Random Forest:\")\n",
    "print(\"    + Best accuracy (usually)\")\n",
    "print(\"    + Robust to overfitting\")\n",
    "print(\"    + Provides feature importance\")\n",
    "print(\"    - Slower training and prediction\")\n",
    "print(\"    - Less interpretable than single tree\")\n",
    "\n",
    "print(\"\\n  K-Nearest Neighbors:\")\n",
    "print(\"    + Simple and intuitive\")\n",
    "print(\"    + No training time (lazy learner)\")\n",
    "print(\"    + Good for non-linear boundaries\")\n",
    "print(\"    - Slow prediction (stores all data)\")\n",
    "print(\"    - Requires feature scaling\")\n",
    "print(\"    - No feature importance\")\n",
    "\n",
    "print(\"\\n  Decision Tree:\")\n",
    "print(\"    + Very interpretable\")\n",
    "print(\"    + Fast training and prediction\")\n",
    "print(\"    + No feature scaling needed\")\n",
    "print(\"    + Provides decision rules\")\n",
    "print(\"    - Prone to overfitting\")\n",
    "print(\"    - Less accurate than RF\")\n",
    "\n",
    "print(\"\\nüí° Use Case Recommendations:\")\n",
    "print(\"  - Production system (accuracy priority): Random Forest\")\n",
    "print(\"  - Real-time detection (speed priority): Decision Tree\")\n",
    "print(\"  - Explainability needed: Decision Tree\")\n",
    "print(\"  - Research/prototyping: Try all and compare\")\n",
    "\n",
    "# Check for overfitting\n",
    "print(\"\\n‚ö†Ô∏è  Overfitting Check:\")\n",
    "for model_name in ['Random Forest', 'KNN', 'Decision Tree']:\n",
    "    gap = comparison_df[comparison_df['Model'] == model_name]['CV-Test Gap'].values[0]\n",
    "    if gap > 0.05:\n",
    "        print(f\"  {model_name}: Large gap ({gap:.4f}) - may be overfitting\")\n",
    "    else:\n",
    "        print(f\"  {model_name}: Good generalization (gap: {gap:.4f})\")\n",
    "\n",
    "logger.info(\"Model comparison completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Export Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comparison results to CSV\n",
    "output_dir = 'results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save comparison DataFrame\n",
    "comparison_df.to_csv(f'{output_dir}/model_comparison.csv', index=False)\n",
    "print(f\"‚úì Results saved to {output_dir}/model_comparison.csv\")\n",
    "\n",
    "# Save detailed results for each model\n",
    "for model_name in ['Random Forest', 'KNN', 'Decision Tree']:\n",
    "    report = results[model_name]['report']\n",
    "    filename = model_name.lower().replace(' ', '_')\n",
    "    with open(f'{output_dir}/{filename}_report.txt', 'w') as f:\n",
    "        f.write(f\"{model_name} Classification Report\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(report)\n",
    "    print(f\"‚úì {model_name} report saved\")\n",
    "\n",
    "print(\"\\n‚úì All results exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Model-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis for specific models\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL-SPECIFIC INSIGHTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Random Forest: Feature importance distribution\n",
    "print(\"\\nüìä Random Forest Feature Importance Distribution:\")\n",
    "rf_model = results['Random Forest']['model'].named_steps['rf']\n",
    "importances = rf_model.feature_importances_\n",
    "print(f\"  Features with >1% importance: {np.sum(importances > 0.01)}\")\n",
    "print(f\"  Top 10 features contribute: {np.sum(sorted(importances, reverse=True)[:10]):.1%}\")\n",
    "print(f\"  Mean importance: {np.mean(importances):.4f}\")\n",
    "\n",
    "# KNN: Distance analysis\n",
    "print(\"\\nüìè KNN Analysis:\")\n",
    "print(f\"  Optimal k: {results['KNN']['optimal_k']}\")\n",
    "print(f\"  Training samples stored: {len(X_train):,}\")\n",
    "print(f\"  Features per sample: {X_train.shape[1]}\")\n",
    "print(f\"  Avg prediction time per sample: {prediction_times['KNN']/len(X_test)*1000:.2f}ms\")\n",
    "\n",
    "# Decision Tree: Depth analysis\n",
    "print(\"\\nüå≥ Decision Tree Structure:\")\n",
    "dt_complexity = results['Decision Tree']['tree_complexity']\n",
    "print(f\"  Depth ratio: {dt_complexity['max_depth']}/{MODEL_CONFIGS['decision_tree']['max_depth']} (actual/max)\")\n",
    "print(f\"  Node efficiency: {dt_complexity['n_leaves']/dt_complexity['n_nodes']:.1%} leaves\")\n",
    "print(f\"  Feature usage: {dt_complexity['n_features_used']}/{X.shape[1]} features used\")\n",
    "\n",
    "if dt_complexity['max_depth'] >= MODEL_CONFIGS['decision_tree']['max_depth']:\n",
    "    print(\"  ‚ö†Ô∏è Tree reached max_depth - consider increasing or pruning\")\n",
    "else:\n",
    "    print(\"  ‚úì Tree stopped before max_depth - good regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Prediction Speed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed timing analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training time\n",
    "models = list(training_times.keys())\n",
    "train_times = list(training_times.values())\n",
    "colors = ['steelblue', 'coral', 'lightgreen']\n",
    "\n",
    "axes[0].barh(models, train_times, color=colors, edgecolor='black')\n",
    "axes[0].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[0].set_title('Training Time Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(train_times):\n",
    "    axes[0].text(v + 0.5, i, f'{v:.2f}s', va='center')\n",
    "\n",
    "# Prediction time per sample\n",
    "pred_per_sample = [prediction_times[m]/len(X_test)*1000 for m in models]\n",
    "axes[1].barh(models, pred_per_sample, color=colors, edgecolor='black')\n",
    "axes[1].set_xlabel('Time per sample (ms)', fontsize=11)\n",
    "axes[1].set_title('Prediction Speed (per sample)', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(pred_per_sample):\n",
    "    axes[1].text(v + 0.01, i, f'{v:.2f}ms', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThroughput Analysis (samples per second):\")\n",
    "for model_name in models:\n",
    "    throughput = len(X_test) / prediction_times[model_name]\n",
    "    print(f\"  {model_name}: {throughput:.0f} samples/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Cross-Validation Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV score stability\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for idx, model_name in enumerate(['Random Forest', 'KNN', 'Decision Tree']):\n",
    "    cv_scores = results[model_name]['cv_scores']\n",
    "    folds = range(1, len(cv_scores) + 1)\n",
    "    \n",
    "    axes[idx].plot(folds, cv_scores, marker='o', markersize=8, \n",
    "                   linewidth=2, color=colors[idx], label='CV Scores')\n",
    "    axes[idx].axhline(y=cv_scores.mean(), color='red', linestyle='--', \n",
    "                      label=f'Mean: {cv_scores.mean():.4f}')\n",
    "    axes[idx].axhline(y=results[model_name]['test_accuracy'], \n",
    "                      color='green', linestyle=':', \n",
    "                      label=f'Test: {results[model_name][\"test_accuracy\"]:.4f}')\n",
    "    axes[idx].set_xlabel('Fold', fontsize=11)\n",
    "    axes[idx].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[idx].set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend(loc='lower right', fontsize=9)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim([cv_scores.min() - 0.01, cv_scores.max() + 0.01])\n",
    "\n",
    "plt.suptitle('Cross-Validation Stability Analysis', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCV Stability (lower std = more stable):\")\n",
    "for model_name in ['Random Forest', 'KNN', 'Decision Tree']:\n",
    "    std = results[model_name]['cv_scores'].std()\n",
    "    stability = \"High\" if std < 0.01 else \"Medium\" if std < 0.02 else \"Low\"\n",
    "    print(f\"  {model_name}: std={std:.4f} ({stability})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance per class\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PER-CLASS PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Parse classification reports to compare per-class performance\n",
    "for model_name in ['Random Forest', 'KNN', 'Decision Tree']:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(results[model_name]['report'])\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Memory and Scalability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MEMORY AND SCALABILITY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Estimate model sizes\n",
    "print(\"\\nüíæ Model Memory Usage:\")\n",
    "\n",
    "# Random Forest\n",
    "rf_model = results['Random Forest']['model']\n",
    "rf_size = sys.getsizeof(rf_model) / 1024**2\n",
    "print(f\"  Random Forest: ~{rf_size:.2f} MB\")\n",
    "print(f\"    ({MODEL_CONFIGS['random_forest']['n_estimators']} trees √ó complexity)\")\n",
    "\n",
    "# KNN (stores all training data)\n",
    "knn_size = X_train.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"  KNN: ~{knn_size:.2f} MB\")\n",
    "print(f\"    (Stores all {len(X_train):,} training samples)\")\n",
    "\n",
    "# Decision Tree\n",
    "dt_model = results['Decision Tree']['model']\n",
    "dt_size = sys.getsizeof(dt_model) / 1024**2\n",
    "print(f\"  Decision Tree: ~{dt_size:.2f} MB\")\n",
    "print(f\"    ({dt_complexity['n_nodes']} nodes)\")\n",
    "\n",
    "print(\"\\nüìà Scalability to Larger Datasets:\")\n",
    "print(\"  Random Forest:\")\n",
    "print(\"    ‚úì Scales well (parallel trees)\")\n",
    "print(\"    ‚úì Can handle millions of samples\")\n",
    "print(\"    ~ Training time: O(n √ó log(n) √ó trees √ó features)\")\n",
    "\n",
    "print(\"\\n  KNN:\")\n",
    "print(\"    ‚ö†Ô∏è Poor scalability\")\n",
    "print(\"    ‚ö†Ô∏è Prediction time grows with dataset size O(n)\")\n",
    "print(\"    ‚ö†Ô∏è Memory usage grows linearly with samples\")\n",
    "print(\"    üí° Consider approximate KNN (FAISS) for >1M samples\")\n",
    "\n",
    "print(\"\\n  Decision Tree:\")\n",
    "print(\"    ‚úì Good scalability for training\")\n",
    "print(\"    ‚úì Fast prediction O(log(n))\")\n",
    "print(\"    ‚ö†Ô∏è May overfit on large datasets without pruning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Production Deployment Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PRODUCTION DEPLOYMENT RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Determine best model for different scenarios\n",
    "best_accuracy = comparison_df.loc[comparison_df['Test Accuracy'].idxmax(), 'Model']\n",
    "fastest_pred = comparison_df.loc[comparison_df['Prediction Time (s)'].idxmin(), 'Model']\n",
    "fastest_train = comparison_df.loc[comparison_df['Training Time (s)'].idxmin(), 'Model']\n",
    "most_stable = comparison_df.loc[comparison_df['CV Std'].idxmin(), 'Model']\n",
    "\n",
    "print(\"\\nüéØ Scenario-Based Recommendations:\\n\")\n",
    "\n",
    "print(\"1Ô∏è‚É£ High-Throughput Real-Time System (e.g., IDS)\")\n",
    "print(f\"   Recommended: {fastest_pred}\")\n",
    "print(f\"   Reason: Fastest prediction ({comparison_df[comparison_df['Model']==fastest_pred]['Prediction Time (s)'].values[0]:.2f}s for {len(X_test):,} samples)\")\n",
    "print(f\"   Accuracy: {comparison_df[comparison_df['Model']==fastest_pred]['Test Accuracy'].values[0]:.4f}\")\n",
    "print(\"   Deployment: Save model, load at startup, minimal latency\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Batch Processing / Offline Analysis\")\n",
    "print(f\"   Recommended: {best_accuracy}\")\n",
    "print(f\"   Reason: Best accuracy ({comparison_df[comparison_df['Model']==best_accuracy]['Test Accuracy'].values[0]:.4f})\")\n",
    "print(\"   Deployment: Can afford longer prediction time for better results\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Frequent Model Retraining\")\n",
    "print(f\"   Recommended: {fastest_train}\")\n",
    "print(f\"   Reason: Fastest training ({comparison_df[comparison_df['Model']==fastest_train]['Training Time (s)'].values[0]:.2f}s)\")\n",
    "print(\"   Use Case: Models retrained hourly/daily with new data\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Explainable AI / Regulatory Compliance\")\n",
    "print(\"   Recommended: Decision Tree\")\n",
    "print(\"   Reason: Fully interpretable decision rules\")\n",
    "print(\"   Use Case: Need to explain why alerts were triggered\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ Mobile/Edge Deployment\")\n",
    "print(\"   Recommended: Decision Tree\")\n",
    "print(f\"   Reason: Smallest model size (~{dt_size:.2f} MB)\")\n",
    "print(\"   Use Case: Embedded systems, IoT devices\")\n",
    "\n",
    "print(\"\\nüí° General Production Checklist:\")\n",
    "print(\"   ‚úÖ Serialize model: Use joblib or pickle\")\n",
    "print(\"   ‚úÖ Version control: Track model versions with metrics\")\n",
    "print(\"   ‚úÖ Input validation: Check feature ranges, handle missing values\")\n",
    "print(\"   ‚úÖ Monitoring: Log predictions, accuracy, latency\")\n",
    "print(\"   ‚úÖ A/B testing: Compare new models against baseline\")\n",
    "print(\"   ‚úÖ Fallback: Have backup model if primary fails\")\n",
    "print(\"   ‚úÖ Update strategy: Plan for model retraining schedule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Save the best model\n",
    "best_model_name = comparison_df.loc[comparison_df['Test Accuracy'].idxmax(), 'Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "best_accuracy_val = results[best_model_name]['test_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Final Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä Results Summary:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüèÜ Winner: {best_model_name}\")\n",
    "print(f\"   Test Accuracy: {best_accuracy_val:.4f}\")\n",
    "print(f\"   CV Score: {results[best_model_name]['cv_scores'].mean():.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. Hyperparameter tuning for best model\")\n",
    "print(\"   2. Feature engineering to improve performance\")\n",
    "print(\"   3. Try ensemble methods (VotingClassifier)\")\n",
    "print(\"   4. Deploy to production environment\")\n",
    "print(\"   5. Set up monitoring and retraining pipeline\")\n",
    "\n",
    "print(\"\\nüí° Improvement Ideas:\")\n",
    "if best_accuracy_val < 0.95:\n",
    "    print(\"   - Accuracy <0.95: Try XGBoost or Neural Networks\")\n",
    "    print(\"   - Increase training data size\")\n",
    "    print(\"   - Perform feature engineering\")\n",
    "elif best_accuracy_val > 0.99:\n",
    "    print(\"   - Accuracy >0.99: Double-check for data leakage!\")\n",
    "    print(\"   - Verify SMOTE is applied correctly\")\n",
    "else:\n",
    "    print(\"   - Good accuracy achieved!\")\n",
    "    print(\"   - Focus on deployment and monitoring\")\n",
    "\n",
    "gap = abs(results[best_model_name]['cv_scores'].mean() - best_accuracy_val)\n",
    "if gap > 0.05:\n",
    "    print(f\"\\n   ‚ö†Ô∏è Large CV-Test gap ({gap:.4f}): Model may be overfitting\")\n",
    "    print(\"   - Increase regularization\")\n",
    "    print(\"   - Reduce model complexity\")\n",
    "    print(\"   - Get more training data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Thank you for using this model comparison framework!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "logger.info(\"Model comparison completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive comparison of three machine learning models:\n",
    "- **Random Forest**: Ensemble method, best accuracy\n",
    "- **K-Nearest Neighbors**: Instance-based, simple but slow\n",
    "- **Decision Tree**: Interpretable, fast, but prone to overfitting\n",
    "\n",
    "### Key Takeaways:\n",
    "1. All models use SMOTE within CV pipeline to prevent data leakage\n",
    "2. Feature scaling is critical for KNN but not for tree-based models\n",
    "3. Random Forest usually provides the best accuracy\n",
    "4. Decision Tree is fastest for both training and prediction\n",
    "5. Check CV-Test gap to detect overfitting\n",
    "\n",
    "### Next Steps:\n",
    "- Try hyperparameter tuning with GridSearchCV\n",
    "- Experiment with other models (XGBoost, Neural Networks)\n",
    "- Perform feature engineering for better performance\n",
    "- Deploy the best model in a production environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
